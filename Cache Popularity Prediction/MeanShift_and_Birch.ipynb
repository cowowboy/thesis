{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MeanShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110415 min 248457 max\n",
      "[['1201639761.78066' '0' '24' ... '24' '8' '1']\n",
      " ['1201639767.58091' '1' '25' ... '30' '87' '1']\n",
      " ['1201639770.34167' '2' '23' ... '5' '4' '1']\n",
      " ...\n",
      " ['1202849965.87344' '80016' '17' ... '151' '997' '13']\n",
      " ['1202849966.01782' '121049' '17' ... '18' '314' '1']\n",
      " ['1202849966.93319' '121050' '22' ... '0' '2' '1']]\n",
      "delete repeat..... (57353, 7)\n",
      "cost time: \n",
      " 0.0\n",
      "request\tvideo\tCluster\thit\tC=5000\n",
      "most cluster: 0\n",
      "len 5000\n",
      "hits [0, 10917]\n",
      "\thitrate [0, 7.908405351955549]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=10000\n",
      "most cluster: 0\n",
      "len 10000\n",
      "hits [0, 10917, 17019]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=15000\n",
      "most cluster: 0\n",
      "len 15000\n",
      "hits [0, 10917, 17019, 20935]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=20000\n",
      "most cluster: 0\n",
      "len 20000\n",
      "hits [0, 10917, 17019, 20935, 24899]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=25000\n",
      "most cluster: 0\n",
      "len 25000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=30000\n",
      "most cluster: 0\n",
      "len 30000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575, 33820]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839, 24.499612439602153]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=35000\n",
      "most cluster: 0\n",
      "len 35000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575, 33820, 36249]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839, 24.499612439602153, 26.25920908702361]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=40000\n",
      "most cluster: 0\n",
      "len 40000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575, 33820, 36249, 38819]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839, 24.499612439602153, 26.25920908702361, 28.120947820606624]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=45000\n",
      "most cluster: 0\n",
      "len 45000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575, 33820, 36249, 38819, 41102]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839, 24.499612439602153, 26.25920908702361, 28.120947820606624, 29.774780322073557]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=50000\n",
      "most cluster: 0\n",
      "len 50000\n",
      "hits [0, 10917, 17019, 20935, 24899, 28575, 33820, 36249, 38819, 41102, 43515]\n",
      "\thitrate [0, 7.908405351955549, 12.32876712328767, 15.16556435313634, 18.037133356997455, 20.7000717167839, 24.499612439602153, 26.25920908702361, 28.120947820606624, 29.774780322073557, 31.52278637815753]\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "def del_repeate(data):\n",
    "    dic = {}\n",
    "    del_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if int(data[i,0]) not in dic:\n",
    "            dic[int(data[i,0])] = 1\n",
    "        else:\n",
    "            del_list.append(i)\n",
    "            \n",
    "    data = np.delete(data,del_list,axis=0)\n",
    "    print(\"delete repeat.....\",data.shape)\n",
    "    return data\n",
    "\n",
    "def view_count(data):\n",
    "    dic = {}\n",
    "    for i in range(data.shape[0]):\n",
    "        if int(data[i,0]) not in dic:\n",
    "            dic[int(data[i,0])] = 1\n",
    "        else:\n",
    "            dic[int(data[i,0])] = dic[int(data[i,0])] + 1\n",
    "    \n",
    "    x = np.zeros((data.shape[0],1), dtype=int)\n",
    "    for i in range(data.shape[0]):\n",
    "        x[i] = dic[int(data[i,0])]\n",
    "        \n",
    "    data = np.concatenate((data,x),axis=1)\n",
    "    print(\"n-1 days viewcount.....\",data.shape)\n",
    "    return data\n",
    "\n",
    "def Data_compare(data,window):\n",
    "    view_count = 0\n",
    "    request_num = 0\n",
    "    start_time = -1\n",
    "    timestamp = 0\n",
    "    end_time = 0\n",
    "    idx = []\n",
    "    for i in range(data.shape[0]):\n",
    "        timestamp = int(data[i,0][3:10])\n",
    "        if start_time == -1:\n",
    "            start_time = timestamp + (window-1) * 86400\n",
    "            end_time = start_time + window * 86400\n",
    "        if end_time < timestamp:\n",
    "\n",
    "            break\n",
    "        if (timestamp >= start_time and timestamp <= end_time):\n",
    "            idx.append(i)\n",
    "        request_num += 1\n",
    "    print(idx[0],'min',idx[-1],'max')\n",
    "    #print(idx)\n",
    "    #print(\"req.num\",request_num)\n",
    "    #data = data[:request_num+1]\n",
    "    #2018/09/21\n",
    "    #data = np.delete(data,[0,1,2,3,5,10],axis=1)\n",
    "    #data = np.delete(data,0,axis=1)\n",
    "\n",
    "    #print(\"%d days shape \"% window,data.shape)\n",
    "    print(data)\n",
    "    return idx\n",
    "\n",
    "def CacheHit(data):\n",
    "    hit = 0\n",
    "    for video_id in data[:,0]:\n",
    "        if video_id in cache_content:\n",
    "            hit += 1\n",
    "    return hit\n",
    "        \n",
    "\n",
    "def UpdateCache(cache_size,url_data,x_label,most_freq):\n",
    "    # == 最多群開啟此行\n",
    "    global cache_content\n",
    "    cache_content.clear()\n",
    "    print(\"most cluster:\",np.argmax(most_freq,axis=0))\n",
    "    while(len(cache_content) < cache_size):\n",
    "        most_cluster = np.argmax(most_freq,axis=0)\n",
    "        #print(\"most_cluster\",most_cluster)\n",
    "        idex = np.where(x_label != most_cluster)[0]\n",
    "        for i in idex:\n",
    "            if url_data[i,0] not in cache_content:\n",
    "                if len(cache_content) < cache_size:\n",
    "                    cache_content.add(url_data[i,0])\n",
    "                else:\n",
    "                    break\n",
    "            #else:\n",
    "                #most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "                    \n",
    "        # != 最多群開啟下面\n",
    "        \n",
    "        idex = np.where(x_label == most_cluster)[0]\n",
    "        for i in idex:\n",
    "            if url_data[i,0] not in cache_content:\n",
    "                if len(cache_content) < cache_size:\n",
    "                    cache_content.add(url_data[i,0])\n",
    "                    most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "\n",
    "    #print(\"length:\",len(cache_content))\n",
    "                \n",
    "\n",
    "def BuildModel(train):\n",
    "    train = np.delete(train,[0],axis=1)\n",
    "    global cluster,X_pred\n",
    "    cluster = []\n",
    "    X = train.astype(np.float32)\n",
    "    #test = test.astype(np.float32)\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X)\n",
    "    X = sc.transform(X)\n",
    "    #test = sc.transform(test)\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "    #test = pca.transform(test)\n",
    "    \n",
    "    #bandwidth = estimate_bandwidth(X , quantile=0.3, n_samples=X.shape[0])\n",
    "    ms = MeanShift(bin_seeding=True,n_jobs=-1)\n",
    "    ms.fit(X)\n",
    "    X_pred = ms.predict(X)\n",
    "    labels = ms.labels_\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    #print(\"y.shape\",y.shape)\n",
    "    \n",
    "    #print(n_clusters_,end=\"\")\n",
    "    cluster = ms.cluster_centers_[:,1]\n",
    "    #print(\"\\nCenter cluster\",cluster)\n",
    "    print(\"model finished!!\")\n",
    "    return X_pred\n",
    "    \n",
    "#all_data = np.concatenate((data_2[1:y.shape[0]+1,:],y.reshape(-1,1)),axis=1)\n",
    "if __name__ == '__main__':  \n",
    "    global cluster,y,cache_content\n",
    "    cache_content = set()\n",
    "    cluster = []\n",
    "    #data_1 = np.loadtxt('./training.txt',dtype='U16',delimiter=' ')\n",
    "    #2018/09/20\n",
    "    data_1 = np.loadtxt('./data_for_meanshift_birch.csv',dtype='U16',delimiter=',')\n",
    "    \n",
    "    test_idx = Data_compare(data_1,7)\n",
    "    #data_new = np.loadtxt('./train_new.txt',dtype='U16',delimiter=' ')\n",
    "    data_1 = np.delete(data_1,0,axis=1)\n",
    "    X_train = data_1[:test_idx[0]]\n",
    "    \n",
    "    #X_train = view_count(X_train)\n",
    "    \n",
    "    X_train = del_repeate(X_train)\n",
    "\n",
    "\n",
    "    X_test = data_1[test_idx[0]:test_idx[-1]+1]\n",
    "    #data_new = Data_preprocess(all_data)\n",
    "\n",
    "    hits = [0]\n",
    "    hit_rate = [0]\n",
    "    hit_idx = 1\n",
    "\n",
    "    index = X_test.shape[0]\n",
    "    \n",
    "    \n",
    "    start_ts = time.time()\n",
    "    #X_pred = BuildModel(X_train) #如果跑第2次就註解掉\n",
    "    end_ts = time.time()\n",
    "    \n",
    "    print(\"cost time: \\n\",end_ts-start_ts)\n",
    "    for j in range(5000,50000+1,5000):\n",
    "        cache_content.clear()\n",
    "        print(\"request\\tvideo\\tCluster\\thit\\tC=\"+str(j))\n",
    "\n",
    "        #hits.append(CacheHit(X_test))\n",
    "        \n",
    "        #print(\"X_pred\",X_pred)\n",
    "        most_item = np.bincount(X_pred) # 有最多元素的cluster\n",
    "        #print(\"most_item\",most_item)\n",
    "        #for i in range(y.shape[0]):\n",
    "            #y[i] = cluster[int(y[i])]\n",
    "        #big = np.amax(cluster)\n",
    "        #ls = sorted(cluster)\n",
    "\n",
    "        UpdateCache(j,X_train,X_pred,most_item)\n",
    "        print(\"len\",len(cache_content))\n",
    "        #BuildModel(X_new,X_test) \n",
    "\n",
    "        hits.append(CacheHit(X_test))\n",
    "        print(\"hits\",hits)\n",
    "        #print(cache_content)\n",
    "        hit_rate.append(hits[hit_idx]/index*100)\n",
    "        hit_idx += 1\n",
    "        print(\"\\thitrate\",hit_rate)\n",
    "\n",
    "\n",
    "        print(\"===============================\")\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110415 min 248457 max\n",
      "[['1201639761.78066' '0' '24' ..., '24' '8' '1']\n",
      " ['1201639767.58091' '1' '25' ..., '30' '87' '1']\n",
      " ['1201639770.34167' '2' '23' ..., '5' '4' '1']\n",
      " ..., \n",
      " ['1202849965.87344' '80016' '17' ..., '151' '997' '13']\n",
      " ['1202849966.01782' '121049' '17' ..., '18' '314' '1']\n",
      " ['1202849966.93319' '121050' '22' ..., '0' '2' '1']]\n",
      "delete repeat..... (57353, 7)\n",
      "['0' '24' '12895' '4' '24' '8' '1']\n",
      "train.shape (57353, 7)\n",
      "test.shape (138043, 1)\n",
      "[3 3 3 ..., 3 3 3]\n",
      "[3 3 3 ..., 3 3 3]\n",
      "model finished!!\n",
      "cost time: \n",
      " 4.014533042907715\n",
      "request\tvideo\tCluster\thit\tC=500\n",
      "most cluster: 3\n",
      "len 500\n",
      "hits [0, 3222]\n",
      "hitrate [0, 2.3340553305853975]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=1000\n",
      "most cluster: 3\n",
      "len 1000\n",
      "hits [0, 3222, 4085]\n",
      "hitrate [0, 2.3340553305853975, 2.959222850850822]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=1500\n",
      "most cluster: 3\n",
      "len 1500\n",
      "hits [0, 3222, 4085, 5023]\n",
      "hitrate [0, 2.3340553305853975, 2.959222850850822, 3.6387212680106926]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=2000\n",
      "most cluster: 3\n",
      "len 2000\n",
      "hits [0, 3222, 4085, 5023, 6013]\n",
      "hitrate [0, 2.3340553305853975, 2.959222850850822, 3.6387212680106926, 4.355889107017378]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=2500\n",
      "most cluster: 3\n",
      "len 2500\n",
      "hits [0, 3222, 4085, 5023, 6013, 6794]\n",
      "hitrate [0, 2.3340553305853975, 2.959222850850822, 3.6387212680106926, 4.355889107017378, 4.9216548466782095]\n",
      "===============================\n",
      "request\tvideo\tCluster\thit\tC=3000\n",
      "most cluster: 3\n",
      "len 3000\n",
      "hits [0, 3222, 4085, 5023, 6013, 6794, 7673]\n",
      "hitrate [0, 2.3340553305853975, 2.959222850850822, 3.6387212680106926, 4.355889107017378, 4.9216548466782095, 5.5584129582811155]\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AffinityPropagation,SpectralClustering,DBSCAN,AgglomerativeClustering,Birch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def del_repeate(data):\n",
    "    dic = {}\n",
    "    del_list = []\n",
    "    for i in range(data.shape[0]):\n",
    "        if int(data[i,0]) not in dic:\n",
    "            dic[int(data[i,0])] = 1\n",
    "        else:\n",
    "            del_list.append(i)\n",
    "            \n",
    "    data = np.delete(data,del_list,axis=0)\n",
    "    print(\"delete repeat.....\",data.shape)\n",
    "    return data\n",
    "\n",
    "\n",
    "def Data_compare(data,window):\n",
    "    view_count = 0\n",
    "    request_num = 0\n",
    "    start_time = -1\n",
    "    timestamp = 0\n",
    "    end_time = 0\n",
    "    idx = []\n",
    "    for i in range(data.shape[0]):\n",
    "        timestamp = int(data[i,0][3:10])\n",
    "        if start_time == -1:\n",
    "            start_time = timestamp + (window-1) * 86400\n",
    "            end_time = start_time + window * 86400\n",
    "        if end_time < timestamp:\n",
    "\n",
    "            break\n",
    "        if (timestamp >= start_time and timestamp <= end_time):\n",
    "            idx.append(i)\n",
    "        request_num += 1\n",
    "    print(idx[0],'min',idx[-1],'max')\n",
    "    #print(idx)\n",
    "    #print(\"req.num\",request_num)\n",
    "    #data = data[:request_num+1]\n",
    "    #2018/09/21\n",
    "    #data = np.delete(data,[0,1,2,3,5,10],axis=1)\n",
    "    #data = np.delete(data,0,axis=1)\n",
    "\n",
    "    #print(\"%d days shape \"% window,data.shape)\n",
    "    print(data)\n",
    "    return idx\n",
    "\n",
    "def CacheHit(data):\n",
    "    hit = 0\n",
    "    for video_id in data[:,0]:\n",
    "        if video_id in cache_content:\n",
    "            hit += 1\n",
    "    return hit\n",
    "\n",
    "\n",
    "def UpdateCache(cache_size,url_data,x_label,most_freq):\n",
    "    # == 最多群開啟此行\n",
    "    global cache_content\n",
    "    cache_content.clear()\n",
    "    print(\"most cluster:\",np.argmax(most_freq,axis=0))\n",
    "    while(len(cache_content) < cache_size):\n",
    "        most_cluster = np.argmax(most_freq,axis=0)\n",
    "        #print(\"most_cluster\",most_cluster)\n",
    "        idex = np.where(x_label != most_cluster)[0]\n",
    "        for i in idex:\n",
    "            if url_data[i,0] not in cache_content:\n",
    "                if len(cache_content) < cache_size:\n",
    "                    cache_content.add(url_data[i,0])\n",
    "                else:\n",
    "                    break\n",
    "            #else:\n",
    "                #most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "                    \n",
    "        # != 最多群開啟下面\n",
    "        \n",
    "        idex = np.where(x_label == most_cluster)[0]\n",
    "        for i in idex:\n",
    "            if url_data[i,0] not in cache_content:\n",
    "                if len(cache_content) < cache_size:\n",
    "                    cache_content.add(url_data[i,0])\n",
    "                    most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                most_freq[most_cluster] = most_freq[most_cluster] - 1\n",
    "\n",
    "    #print(\"length:\",len(cache_content))\n",
    "                \n",
    "\n",
    "def BuildModel(train):\n",
    "    train = np.delete(train,[0],axis=1)\n",
    "    #test = np.delete(test,[0],axis=1)\n",
    "    cluster = []\n",
    "    X = train.astype(np.float32)\n",
    "    #test = test.astype(np.float32)\n",
    "    sc = StandardScaler()\n",
    "    sc.fit(X)\n",
    "    X = sc.transform(X)\n",
    "    #test = sc.transform(test)\n",
    "    pca = PCA(n_components=4)\n",
    "    pca.fit(X)\n",
    "    X = pca.transform(X)\n",
    "    #test = pca.transform(test)\n",
    "\n",
    "    af = Birch(n_clusters=6)\n",
    "    #af.fit(X)\n",
    "    X_pred = af.fit_predict(X)\n",
    "    #center = af.subcluster_centers_\n",
    "    #print(\"####################\",center.shape)\n",
    "    labels = af.labels_\n",
    "    print(labels)\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    print(X_pred)\n",
    "    #y = af.fit_predict(test)\n",
    "    \n",
    "    print(\"model finished!!\")\n",
    "    return X_pred\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':  \n",
    "    global cluster,y,cache_content\n",
    "    cache_content = set()\n",
    "    cluster = []\n",
    "\n",
    "    data_1 = np.loadtxt('./data_for_meanshift_birch.csv',dtype='U16',delimiter=',')\n",
    "    \n",
    "    test_idx = Data_compare(data_1,7)\n",
    "    data_1 = np.delete(data_1,0,axis=1)\n",
    "    X_train = data_1[:test_idx[0]]\n",
    "    #X_train = data_new[:train_samples,]\n",
    "\n",
    "    X_train = del_repeate(X_train)\n",
    "    \n",
    "    print(X_train[0])\n",
    "    #X_update = data_new[-69616:-11051]\n",
    "    X_test = data_1[test_idx[0]:test_idx[-1]+1,[0]]\n",
    "    print(\"train.shape\",X_train.shape)\n",
    "    #print(X_update.shape)\n",
    "    print(\"test.shape\",X_test.shape)\n",
    "\n",
    "    hits = [0]\n",
    "    hit_rate = [0]\n",
    "    hit_idx = 1\n",
    "\n",
    "    index = X_test.shape[0]\n",
    "    \n",
    "    start_ts = time.time()\n",
    "    X_pred = BuildModel(X_train) #如果跑第2次就註解掉\n",
    "    end_ts = time.time()\n",
    "    \n",
    "    print(\"cost time: \\n\",end_ts-start_ts)\n",
    "    \n",
    "    for j in range(500,3000+1,500):\n",
    "        cache_content.clear()\n",
    "        print(\"request\\tvideo\\tCluster\\thit\\tC=\"+str(j))\n",
    "\n",
    "        most_item = np.bincount(X_pred)\n",
    "        \n",
    "        UpdateCache(j,X_train,X_pred,most_item)\n",
    "        print(\"len\",len(cache_content))\n",
    "\n",
    "\n",
    "        hits.append(CacheHit(X_test))\n",
    "        print(\"hits\",hits)\n",
    "        hit_rate.append(hits[hit_idx]/index*100)\n",
    "        hit_idx += 1\n",
    "        print(\"hitrate\",hit_rate)\n",
    "\n",
    "\n",
    "        print(\"===============================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
