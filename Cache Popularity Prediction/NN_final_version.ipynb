{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110415 min 248457 max\n",
      "(110415, 8)\n",
      "(138042,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#L = 7 or 14\n",
    "L = 7\n",
    "\n",
    "# Pick out L-days data to training data\n",
    "def Data_compare(data,window):\n",
    "    view_count = 0\n",
    "    request_num = 0\n",
    "    start_time = -1\n",
    "    timestamp = 0\n",
    "    end_time = 0\n",
    "    idx = []\n",
    "    for i in range(data.shape[0]):\n",
    "        timestamp = int(str(data.iloc[i,0])[3:10])\n",
    "        if start_time == -1:\n",
    "            start_time = timestamp + (window-1) * 86400\n",
    "            end_time = start_time + window * 86400\n",
    "        if end_time < timestamp:\n",
    "            break\n",
    "        if (timestamp >= start_time and timestamp <= end_time):\n",
    "            idx.append(i)\n",
    "    print(idx[0],'min',idx[-1],'max')\n",
    "    return idx\n",
    "\n",
    "data = pd.read_csv(\"./test_20180920 _orgin\")\n",
    "data[\"Video_ID\"] = pd.Categorical(data['Video_ID'], categories=data[\"Video_ID\"].unique()).codes\n",
    "data = data[['Req_time','Video_ID','Video_Category','Like_Count','Dislike_Count','Comment_Count','Local_View','Video_Count']]\n",
    "idx = Data_compare(data,L)\n",
    "\n",
    "train_data = data.iloc[:idx[0]]\n",
    "test_data = data.iloc[idx[0]:idx[-1],1]\n",
    "test_data.to_csv(\"./testing_1001_video.csv\",index=0)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove \"View Count = 0\" Rows and Duplicate Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57353, 8)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.drop_duplicates(subset=['Video_ID'], keep=\"first\")\n",
    "print(train_data.shape)\n",
    "train_data = train_data[train_data.Video_Count != 0]\n",
    "train_data[\"Video_ID\"].to_csv(\"./training_1001_video.csv\",index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57350, 8)\n",
      "(57350, 7)\n",
      "(57350, 24)\n",
      "[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.11890336e-05 1.74361836e-05 8.54023413e-05\n",
      " 6.86487859e-05 2.38545694e-04 0.00000000e+00]\n",
      "(57350, 23)\n",
      "(57350, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wmnet\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_function_transformer.py:98: FutureWarning: The default validate=True will be replaced by validate=False in 0.22.\n",
      "  \"validate=False in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "onehot_encoding = pd.get_dummies(train_data['Video_Category'],prefix='Video_Category')\n",
    "print(train_data.shape)\n",
    "X_train = train_data.drop(['Video_Category'],axis=1)\n",
    "print(X_train.shape)\n",
    "X_train = pd.concat([onehot_encoding,X_train],axis=1)\n",
    "print(X_train.shape)\n",
    "\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "\n",
    "# MinMax standardization\n",
    "scaler = MinMaxScaler()\n",
    "#scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train.drop(['Video_Count'],axis=1).as_matrix())\n",
    "#y_train = scaler.fit_transform(X_train['Video_Count'].astype(int).as_matrix().reshape(X_train.shape[0],1))\n",
    "\n",
    "y_train = transformer.transform(X_train['Video_Count'].astype(int).as_matrix().reshape(1,-1))\n",
    "y_train = y_train.reshape(-1,1)\n",
    "print(x_train[1])\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "#original_data = train_data.drop(original_data.columns[1],axis=1)\n",
    "#original_data = np.delete(original_data,[0,-1],axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 931601.3750\n",
      "Epoch : 0 Cost : 27.649727\n",
      "Step 1000, Minibatch Loss= 22.5020\n",
      "Epoch : 1 Cost : 26.115046\n",
      "Step 2000, Minibatch Loss= 26.6270\n",
      "Epoch : 2 Cost : 24.711737\n",
      "Step 3000, Minibatch Loss= 22.4074\n",
      "Epoch : 3 Cost : 23.45364\n",
      "Step 4000, Minibatch Loss= 21.9638\n",
      "Epoch : 4 Cost : 22.298676\n",
      "Step 5000, Minibatch Loss= 19.7384\n",
      "Epoch : 5 Cost : 21.328468\n",
      "Step 6000, Minibatch Loss= 18.2394\n",
      "Epoch : 6 Cost : 20.244238\n",
      "Step 7000, Minibatch Loss= 25.7811\n",
      "Epoch : 7 Cost : 19.398735\n",
      "Step 8000, Minibatch Loss= 19.7897\n",
      "Epoch : 8 Cost : 18.48216\n",
      "Epoch : 9 Cost : 17.72505\n",
      "Step 9000, Minibatch Loss= 13.9345\n",
      "Epoch : 10 Cost : 17.018612\n",
      "Step 10000, Minibatch Loss= 13.7821\n",
      "Epoch : 11 Cost : 16.27776\n",
      "Step 11000, Minibatch Loss= 15.8305\n",
      "Epoch : 12 Cost : 15.642202\n",
      "Step 12000, Minibatch Loss= 12.6220\n",
      "Epoch : 13 Cost : 15.061877\n",
      "Step 13000, Minibatch Loss= 13.0732\n",
      "Epoch : 14 Cost : 14.485539\n",
      "Step 14000, Minibatch Loss= 11.3291\n",
      "Epoch : 15 Cost : 13.979897\n",
      "Step 15000, Minibatch Loss= 17.6708\n",
      "Epoch : 16 Cost : 13.478897\n",
      "Step 16000, Minibatch Loss= 12.9425\n",
      "Epoch : 17 Cost : 13.021446\n",
      "Step 17000, Minibatch Loss= 11.6666\n",
      "Epoch : 18 Cost : 12.633084\n",
      "Epoch : 19 Cost : 12.211295\n",
      "Step 18000, Minibatch Loss= 10.3391\n",
      "Epoch : 20 Cost : 11.815832\n",
      "Step 19000, Minibatch Loss= 12.2744\n",
      "Epoch : 21 Cost : 11.464541\n",
      "Step 20000, Minibatch Loss= 8.3757\n",
      "Epoch : 22 Cost : 11.138927\n",
      "Step 21000, Minibatch Loss= 8.2263\n",
      "Epoch : 23 Cost : 10.856883\n",
      "Step 22000, Minibatch Loss= 13.0081\n",
      "Epoch : 24 Cost : 10.524103\n",
      "Step 23000, Minibatch Loss= 10.9079\n",
      "Epoch : 25 Cost : 10.248247\n",
      "Step 24000, Minibatch Loss= 10.5922\n",
      "Epoch : 26 Cost : 10.028687\n",
      "Step 25000, Minibatch Loss= 7.9475\n",
      "Epoch : 27 Cost : 9.883936\n",
      "Epoch : 28 Cost : 9.528786\n",
      "Step 26000, Minibatch Loss= 9.8401\n",
      "Epoch : 29 Cost : 9.312391\n",
      "Step 27000, Minibatch Loss= 7.2516\n",
      "Epoch : 30 Cost : 9.103228\n",
      "Step 28000, Minibatch Loss= 9.9842\n",
      "Epoch : 31 Cost : 8.943378\n",
      "Step 29000, Minibatch Loss= 7.6344\n",
      "Epoch : 32 Cost : 8.73588\n",
      "Step 30000, Minibatch Loss= 6.4249\n",
      "Epoch : 33 Cost : 8.596328\n",
      "Step 31000, Minibatch Loss= 7.5709\n",
      "Epoch : 34 Cost : 8.412121\n",
      "Step 32000, Minibatch Loss= 7.2954\n",
      "Epoch : 35 Cost : 8.272865\n",
      "Step 33000, Minibatch Loss= 8.1367\n",
      "Epoch : 36 Cost : 8.159484\n",
      "Step 34000, Minibatch Loss= 7.5764\n",
      "Epoch : 37 Cost : 7.993812\n",
      "Epoch : 38 Cost : 7.8722773\n",
      "Step 35000, Minibatch Loss= 7.1070\n",
      "Epoch : 39 Cost : 7.755541\n",
      "Step 36000, Minibatch Loss= 7.8728\n",
      "Epoch : 40 Cost : 7.6493907\n",
      "Step 37000, Minibatch Loss= 8.0232\n",
      "Epoch : 41 Cost : 7.563973\n",
      "Step 38000, Minibatch Loss= 8.1586\n",
      "Epoch : 42 Cost : 7.449861\n",
      "Step 39000, Minibatch Loss= 7.5307\n",
      "Epoch : 43 Cost : 7.362463\n",
      "Step 40000, Minibatch Loss= 7.5810\n",
      "Epoch : 44 Cost : 7.2879405\n",
      "Step 41000, Minibatch Loss= 5.6434\n",
      "Epoch : 45 Cost : 7.1962533\n",
      "Step 42000, Minibatch Loss= 6.6633\n",
      "Epoch : 46 Cost : 7.1236615\n",
      "Step 43000, Minibatch Loss= 7.2788\n",
      "Epoch : 47 Cost : 7.062221\n",
      "Epoch : 48 Cost : 6.986688\n",
      "Step 44000, Minibatch Loss= 8.4571\n",
      "Epoch : 49 Cost : 6.9205275\n",
      "Step 45000, Minibatch Loss= 7.0433\n",
      "Epoch : 50 Cost : 6.865122\n",
      "Step 46000, Minibatch Loss= 7.2795\n",
      "Epoch : 51 Cost : 6.8188424\n",
      "Step 47000, Minibatch Loss= 7.9157\n",
      "Epoch : 52 Cost : 6.7637424\n",
      "Step 48000, Minibatch Loss= 5.2010\n",
      "Epoch : 53 Cost : 6.708946\n",
      "Step 49000, Minibatch Loss= 7.1756\n",
      "Epoch : 54 Cost : 6.6677933\n",
      "Step 50000, Minibatch Loss= 7.6180\n",
      "Epoch : 55 Cost : 6.634382\n",
      "Step 51000, Minibatch Loss= 7.0703\n",
      "Epoch : 56 Cost : 6.5729127\n",
      "Epoch : 57 Cost : 6.535889\n",
      "Step 52000, Minibatch Loss= 7.2333\n",
      "Epoch : 58 Cost : 6.527312\n",
      "Step 53000, Minibatch Loss= 5.7680\n",
      "Epoch : 59 Cost : 6.4736357\n",
      "Step 54000, Minibatch Loss= 5.5388\n",
      "Epoch : 60 Cost : 6.429785\n",
      "Step 55000, Minibatch Loss= 5.0478\n",
      "Epoch : 61 Cost : 6.4263773\n",
      "Step 56000, Minibatch Loss= 5.4084\n",
      "Epoch : 62 Cost : 6.373798\n",
      "Step 57000, Minibatch Loss= 5.7108\n",
      "Epoch : 63 Cost : 6.3535137\n",
      "Step 58000, Minibatch Loss= 6.5473\n",
      "Epoch : 64 Cost : 6.331657\n",
      "Step 59000, Minibatch Loss= 6.4203\n",
      "Epoch : 65 Cost : 6.305715\n",
      "Step 60000, Minibatch Loss= 7.4325\n",
      "Epoch : 66 Cost : 6.3213677\n",
      "Epoch : 67 Cost : 6.262265\n",
      "Step 61000, Minibatch Loss= 6.1368\n",
      "Epoch : 68 Cost : 6.22952\n",
      "Step 62000, Minibatch Loss= 6.2650\n",
      "Epoch : 69 Cost : 6.2087326\n",
      "Step 63000, Minibatch Loss= 6.3431\n",
      "Epoch : 70 Cost : 6.1907897\n",
      "Step 64000, Minibatch Loss= 6.0251\n",
      "Epoch : 71 Cost : 6.1733146\n",
      "Step 65000, Minibatch Loss= 5.5305\n",
      "Epoch : 72 Cost : 6.1721673\n",
      "Step 66000, Minibatch Loss= 6.4982\n",
      "Epoch : 73 Cost : 6.1450386\n",
      "Step 67000, Minibatch Loss= 6.7252\n",
      "Epoch : 74 Cost : 6.1295094\n",
      "Step 68000, Minibatch Loss= 8.6061\n",
      "Epoch : 75 Cost : 6.1174264\n",
      "Epoch : 76 Cost : 6.105914\n",
      "Step 69000, Minibatch Loss= 3.7802\n",
      "Epoch : 77 Cost : 6.086685\n",
      "Step 70000, Minibatch Loss= 5.8365\n",
      "Epoch : 78 Cost : 6.080106\n",
      "Step 71000, Minibatch Loss= 6.9486\n",
      "Epoch : 79 Cost : 6.0695915\n",
      "Step 72000, Minibatch Loss= 6.8310\n",
      "Epoch : 80 Cost : 6.0666466\n",
      "Step 73000, Minibatch Loss= 6.0983\n",
      "Epoch : 81 Cost : 6.071482\n",
      "Step 74000, Minibatch Loss= 5.5464\n",
      "Epoch : 82 Cost : 6.0339217\n",
      "Step 75000, Minibatch Loss= 5.7194\n",
      "Epoch : 83 Cost : 6.0225067\n",
      "Step 76000, Minibatch Loss= 5.1860\n",
      "Epoch : 84 Cost : 6.0232563\n",
      "Step 77000, Minibatch Loss= 5.1147\n",
      "Epoch : 85 Cost : 6.0047603\n",
      "Epoch : 86 Cost : 6.004967\n",
      "Step 78000, Minibatch Loss= 4.9471\n",
      "Epoch : 87 Cost : 5.995368\n",
      "Step 79000, Minibatch Loss= 4.8946\n",
      "Epoch : 88 Cost : 5.981834\n",
      "Step 80000, Minibatch Loss= 7.9437\n",
      "Epoch : 89 Cost : 5.975027\n",
      "Step 81000, Minibatch Loss= 4.4246\n",
      "Epoch : 90 Cost : 5.9838753\n",
      "Step 82000, Minibatch Loss= 7.1794\n",
      "Epoch : 91 Cost : 5.961857\n",
      "Step 83000, Minibatch Loss= 4.4120\n",
      "Epoch : 92 Cost : 5.9662175\n",
      "Step 84000, Minibatch Loss= 6.4443\n",
      "Epoch : 93 Cost : 5.963244\n",
      "Step 85000, Minibatch Loss= 5.6228\n",
      "Epoch : 94 Cost : 5.946207\n",
      "Step 86000, Minibatch Loss= 5.5937\n",
      "Epoch : 95 Cost : 5.940792\n",
      "Epoch : 96 Cost : 5.935556\n",
      "Step 87000, Minibatch Loss= 4.4973\n",
      "Epoch : 97 Cost : 5.92768\n",
      "Step 88000, Minibatch Loss= 6.4359\n",
      "Epoch : 98 Cost : 5.923079\n",
      "Step 89000, Minibatch Loss= 6.6450\n",
      "Epoch : 99 Cost : 5.9488277\n"
     ]
    }
   ],
   "source": [
    "def neural_net_model(X_data,input_dim):\n",
    "    W_1 = tf.Variable(tf.random_uniform([input_dim,100]))\n",
    "    b_1 = tf.Variable(tf.zeros([100]))\n",
    "    layer_1 = tf.add(tf.matmul(X_data,W_1), b_1)\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # layer 1 multiplying and adding bias then activation function\n",
    "    W_2 = tf.Variable(tf.random_uniform([100,100]))\n",
    "    b_2 = tf.Variable(tf.zeros([100]))\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,W_2), b_2)\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # layer 2 multiplying and adding bias then activation function\n",
    "    W_O = tf.Variable(tf.random_uniform([100,1]))\n",
    "    b_O = tf.Variable(tf.zeros([1]))\n",
    "    output = tf.add(tf.matmul(layer_2,W_O), b_O)\n",
    "    # O/p layer multiplying and adding bias then activation function\n",
    "    # notice output layer has one node only since performing #regression\n",
    "    return output\n",
    "\n",
    "\"\"\"\n",
    "neural_net_model is function applying 2 hidden layer feed forward neural net.\n",
    "Weights and biases are abberviated as W_1,W_2 and b_1, b_2 \n",
    "These are variables with will be updated during training.\n",
    "\"\"\"\n",
    "#xs = tf.placeholder(\"float\")\n",
    "#ys = tf.placeholder(\"float\")\n",
    "batch_size = 64\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "display_step = 1000\n",
    "c_t = []\n",
    "\n",
    "\n",
    "xs = tf.placeholder(tf.float32, [None,23])\n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "output = neural_net_model(xs,23)\n",
    "cost = tf.reduce_mean(tf.square(output-ys))\n",
    "# our mean squared error cost function\n",
    "train = tf.train.GradientDescentOptimizer(1e-6).minimize(cost,global_step=global_step)\n",
    "# Gradinent Descent optimiztion just discussed above for updating weights and biases\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initiate session and initialize all vaiables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    #saver.restore(sess,'yahoo_dataset.ckpt')\n",
    "    for i in range(100):\n",
    "        permutation=np.random.permutation(x_train.shape[0])\n",
    "        x_batch = x_train[permutation,:]\n",
    "        y_batch = y_train[permutation]\n",
    "        x_batch = np.array_split(x_batch, x_train.shape[0] // batch_size)\n",
    "        y_batch = np.array_split(y_batch, x_train.shape[0] // batch_size)\n",
    "        #print(x_batch[0].shape)\n",
    "        #print(y_batch[0].shape)\n",
    "        for batch_x,batch_y in zip(x_batch,y_batch):\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train, feed_dict={xs: batch_x, ys: batch_y.reshape(batch_y.shape[0],1)})\n",
    "            step = sess.run(global_step)\n",
    "            if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss = sess.run(cost, feed_dict={xs: batch_x,ys: batch_y.reshape(batch_y.shape[0],1)})\n",
    "                print(\"Step \" + str(step) + \", Minibatch Loss= \" +\"{:.4f}\".format(loss))\n",
    "    \n",
    "        #for j in range(x_train.shape[0]):\n",
    "            #sess.run([cost,train],feed_dict={xs:x_train[j,:].reshape(1,23), ys:y_train[j]})\n",
    "            # Run cost and train with each sample\n",
    "        c_t.append(sess.run(cost, feed_dict={xs:x_train,ys:y_train.reshape(y_train.shape[0],1)}))\n",
    "        #c_test.append(sess.run(cost, feed_dict={xs:X_test,ys:y_test}))\n",
    "        print('Epoch :',i,'Cost :',c_t[i])\n",
    "    pred = sess.run(output, feed_dict={xs:x_train})\n",
    "    # predict output of test data after training\n",
    "    #print('Cost :',sess.run(cost, feed_dict={xs:X_test,ys:y_test}))\n",
    "    #y_test = denormalize(df_test,y_test)\n",
    "    #pred = denormalize(df_test,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predicted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"pred_2018_10_10.csv\",pred, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort and Calculate Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               0      1\n",
      "48     31.095615     48\n",
      "20713  29.874971  20714\n",
      "4687   25.445910   4687\n",
      "2906   24.593761   2906\n",
      "4688   23.266281   4688\n",
      "53679  21.101404  53681\n",
      "45824  19.797440  45826\n",
      "4701   18.819902   4701\n",
      "5357   18.592070   5357\n",
      "1188   17.982145   1188\n",
      "3982   17.923649   3982\n",
      "11654  17.592016  11655\n",
      "48455  17.366436  48457\n",
      "3556   17.358101   3556\n",
      "2989   17.212898   2989\n",
      "58     17.019949     58\n",
      "36950  16.775280  36951\n",
      "14404  16.578680  14405\n",
      "3577   16.228876   3577\n",
      "3395   16.226210   3395\n",
      "56296  16.196896  56298\n",
      "21978  16.195562  21979\n",
      "53803  16.161816  53805\n",
      "7221   16.026272   7221\n",
      "3703   15.857057   3703\n",
      "8095   15.686289   8096\n",
      "3053   15.600843   3053\n",
      "2276   15.493347   2276\n",
      "3003   15.449720   3003\n",
      "9949   15.285483   9950\n",
      "...          ...    ...\n",
      "6217    7.799737   6217\n",
      "2061    7.798448   2061\n",
      "6052    7.797279   6052\n",
      "4614    7.796119   4614\n",
      "962     7.795404    962\n",
      "5841    7.794765   5841\n",
      "5933    7.794611   5933\n",
      "4601    7.785686   4601\n",
      "3285    7.785018   3285\n",
      "4591    7.783528   4591\n",
      "3633    7.777079   3633\n",
      "286     7.776753    286\n",
      "2406    7.752274   2406\n",
      "2339    7.751614   2339\n",
      "1264    7.746357   1264\n",
      "959     7.746043    959\n",
      "2013    7.743948   2013\n",
      "976     7.743433    976\n",
      "1479    7.741603   1479\n",
      "1492    7.739421   1492\n",
      "385     7.737382    385\n",
      "1004    7.734289   1004\n",
      "938     7.730933    938\n",
      "633     7.729109    633\n",
      "873     7.728782    873\n",
      "453     7.728403    453\n",
      "475     7.720139    475\n",
      "478     7.719996    478\n",
      "270     7.719129    270\n",
      "142     7.713558    142\n",
      "\n",
      "[57350 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wmnet\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv(\"pred_2018_10_10.csv\",header=None)\n",
    "train_ID = pd.read_csv(\"training_1001_video.csv\",header=None)\n",
    "test_ID = pd.read_csv(\"testing_1001_video.csv\",header=None)\n",
    "final_result = pd.concat([result,train_ID],axis=1,ignore_index=True)\n",
    "final_result = final_result.sort([0],ascending=False)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cach Size 100.000000 ---Hit Rate: 1.045334\n"
     ]
    }
   ],
   "source": [
    "total_size = test_ID.shape[0]\n",
    "# cache size range 500,5000,50000\n",
    "for length in range(100,100+1,500):\n",
    "    hit = 0\n",
    "    cache_cotainer = final_result.iloc[:length+1,1]\n",
    "    for item in test_ID.iloc[:,0]:\n",
    "        if item in cache_cotainer:\n",
    "            hit += 1\n",
    "    print(\"Cach Size %5f ---Hit Rate: %f\" % (length,hit/total_size*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
